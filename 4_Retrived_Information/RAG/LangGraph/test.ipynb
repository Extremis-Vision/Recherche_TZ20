{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368bd0e7",
   "metadata": {},
   "source": [
    "# RAG System with Smart Routing\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system with the following components:\n",
    "\n",
    "1. **Document Loading**: Loading blog posts about AI/ML topics from Lilian Weng's blog\n",
    "2. **Vector Storage**: Using SKLearn vector store with Nomic embeddings for document storage\n",
    "3. **Smart Router**: A system that decides whether to use:\n",
    "   - Vector store: For questions about agents, prompt engineering, and adversarial attacks\n",
    "   - Web search: For current events and topics not in the knowledge base\n",
    "\n",
    "The system demonstrates how to intelligently route questions to the appropriate data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d5182",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     18\u001b[39m doc_splits = text_splitter.split_documents(docs_list)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Add to vectorDB\u001b[39;00m\n\u001b[32m     21\u001b[39m vectorstore = Chroma.from_documents(\n\u001b[32m     22\u001b[39m     documents=doc_splits,\n\u001b[32m     23\u001b[39m     collection_name=\u001b[33m\"\u001b[39m\u001b[33mrag-chroma\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     embedding=\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     26\u001b[39m retriever = vectorstore.as_retriever()\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/function_calling/venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:326\u001b[39m, in \u001b[36mOpenAIEmbeddings.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_client = httpx.Client(proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy)\n\u001b[32m    325\u001b[39m     sync_specific = {\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_client}\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m.embeddings  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client:\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.openai_proxy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.http_async_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/function_calling/venv/lib/python3.12/site-packages/openai/_client.py:116\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    114\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        api_key=\"lm-studio\",\n",
    "        base_url=\"http://localhost:1234/v1\",\n",
    "        model=\"gemma-3-4b-it\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=4096\n",
    "    )\n",
    "\n",
    "\n",
    "llm_json_mode = ChatOpenAI(\n",
    "        api_key=\"lm-studio\",\n",
    "        base_url=\"http://localhost:1234/v1\",\n",
    "        model=\"gemma-3-4b-it\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=4096, \n",
    "        format=\"json\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006e8453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Embedding texts: 100%|██████████| 47/47 [01:03<00:00,  1.35s/inputs]\n",
      "Embedding texts: 100%|██████████| 47/47 [01:03<00:00,  1.35s/inputs]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315231c",
   "metadata": {},
   "source": [
    "## Router Implementation\n",
    "\n",
    "The router decides between two data sources:\n",
    "1. **Vector Store**: For questions about content in our knowledge base\n",
    "2. **Web Search**: For current events and topics outside our knowledge base\n",
    "\n",
    "The router uses the LLM in JSON mode to return a structured decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fbf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router Test Results:\n",
      "--------------------------------------------------\n",
      "Warning: Routing failed - Completions.create() got an unexpected keyword argument 'format'\n",
      "Question: Who is favored to win the NFC Championship game in the 2024 season?\n",
      "Routed to: websearch\n",
      "\n",
      "Warning: Routing failed - Completions.create() got an unexpected keyword argument 'format'\n",
      "Question: What are the models released today for llama3.2?\n",
      "Routed to: websearch\n",
      "\n",
      "Warning: Routing failed - Completions.create() got an unexpected keyword argument 'format'\n",
      "Question: What are the types of agent memory ?\n",
      "Routed to: websearch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Literal\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "@dataclass\n",
    "class RouterConfig:\n",
    "    vectorstore_topics: List[str] = (\"agents\", \"prompt engineering\", \"adversarial attacks\")\n",
    "    system_prompt: str = \"\"\"You are an expert at routing questions to the appropriate data source.\n",
    "    \n",
    "    RULES:\n",
    "    1. The vectorstore contains documents about: {topics}\n",
    "    2. Use vectorstore ONLY for questions about these topics\n",
    "    3. Use websearch for everything else, especially current events\n",
    "    \n",
    "    Return a JSON object with format: {{\"datasource\": \"websearch\"}} or {{\"datasource\": \"vectorstore\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_prompt(self) -> str:\n",
    "        return self.system_prompt.format(topics=\", \".join(self.vectorstore_topics))\n",
    "\n",
    "class QuestionRouter:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.config = RouterConfig()\n",
    "    \n",
    "    def route(self, question: str) -> Dict[str, Literal[\"websearch\", \"vectorstore\"]]:\n",
    "    \"\"\"Route a single question to appropriate data source.\"\"\"\n",
    "    try:\n",
    "        print(f\"Envoi de la requête à LM-Studio: {question}\")\n",
    "        response = self.llm.invoke([\n",
    "            SystemMessage(content=self.config.get_prompt()),\n",
    "            HumanMessage(content=question)\n",
    "        ])\n",
    "        print(f\"Réponse reçue: {response.content}\")\n",
    "        return json.loads(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Routing failed - {str(e)}\")\n",
    "        print(f\"Trace complète:\", traceback.format_exc())\n",
    "        return {\"datasource\": \"websearch\"}\n",
    "\n",
    "# Create router instance\n",
    "router = QuestionRouter(llm_json_mode)\n",
    "\n",
    "# Test cases\n",
    "test_questions = [\n",
    "    \"Who is favored to win the NFC Championship game in the 2024 season?\",  # Should be websearch\n",
    "    \"What are the models released today for llama3.2?\",                     # Should be websearch\n",
    "    \"What are the types of agent memory?\"                                   # Should be vectorstore\n",
    "]\n",
    "\n",
    "# Run tests and display results\n",
    "print(\"Router Test Results:\")\n",
    "print(\"-\" * 50)\n",
    "for question in test_questions:\n",
    "    result = router.route(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Routed to: {result['datasource']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
