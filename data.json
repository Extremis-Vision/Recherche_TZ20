{
    "results": [
        {
            "model_name": "gemma-3-12b-it",
            "temperature": 0.7,
            "max_tokens": 4096,
            "attempts": 10,
            "time": [
                {
                    "chargement_model": 4.347776889801025
                },
                {
                    "reponse_time_0": 4.34777569770813
                },
                {
                    "chargement_model": 8.515415668487549
                },
                {
                    "reponse_time_1": 4.167588472366333
                },
                {
                    "chargement_model": 12.15790343284607
                },
                {
                    "chargement_model": 15.905227422714233
                },
                {
                    "chargement_model": 19.89888310432434
                },
                {
                    "reponse_time_4": 3.9936163425445557
                },
                {
                    "chargement_model": 23.389947414398193
                },
                {
                    "chargement_model": 27.822066068649292
                },
                {
                    "reponse_time_6": 4.432059049606323
                },
                {
                    "chargement_model": 31.744271278381348
                },
                {
                    "reponse_time_7": 3.9221391677856445
                },
                {
                    "chargement_model": 35.84398436546326
                },
                {
                    "reponse_time_8": 4.099628686904907
                },
                {
                    "chargement_model": 39.56425976753235
                },
                {
                    "reponse_time_9": 3.7201948165893555
                },
                {
                    "avg_reponse_time": 4.097571747643607
                },
                {
                    "total_time": 39.56431531906128
                }
            ],
            "score": 7,
            "result": "[{'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations language models', 'LLM architecture alternatives', 'hybrid architectures LLMs CNN', 'transformer networks vs CNNs'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architectures alternatives', 'hybrid CNN-Transformer models', 'why not CNN for language models'], 'categories': 'AI Research'}, {'mot_cle': ['CNN for LLMs', 'Convolutional Neural Networks Language Models', 'LLM architecture limitations', 'Alternatives to Transformers LLMs', 'Hybrid neural network models'], 'categories': 'AI Research'}, {'mot_cle': ['CNN LLMs', 'Convolutional Neural Networks Language Models', 'LLM Architecture Alternatives', 'Hybrid LLM Architectures', 'Limitations of CNNs in LLMs'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'transformer network advantages', 'hybrid CNN-Transformer models'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'hybrid architectures LLMs CNN', 'transformer networks vs CNNs'], 'categories': 'AI Research'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'transformer vs CNN language models', 'hybrid architectures LLMs'], 'categories': 'AI Research'}]"
        },
        {
            "model_name": "gemma-3-12b-it",
            "temperature": 0.7,
            "max_tokens": 4096,
            "attempts": 10,
            "time": [
                {
                    "chargement_model": 3.577526092529297
                },
                {
                    "chargement_model": 7.717522859573364
                },
                {
                    "reponse_time_1": 4.139974594116211
                },
                {
                    "chargement_model": 11.679481744766235
                },
                {
                    "reponse_time_2": 3.9619181156158447
                },
                {
                    "chargement_model": 15.521738767623901
                },
                {
                    "reponse_time_3": 3.8422038555145264
                },
                {
                    "chargement_model": 19.432642221450806
                },
                {
                    "reponse_time_4": 3.910858392715454
                },
                {
                    "chargement_model": 23.41322135925293
                },
                {
                    "reponse_time_5": 3.9805402755737305
                },
                {
                    "chargement_model": 27.754616022109985
                },
                {
                    "reponse_time_6": 4.34135365486145
                },
                {
                    "chargement_model": 32.17344856262207
                },
                {
                    "reponse_time_7": 4.418792724609375
                },
                {
                    "chargement_model": 36.088046073913574
                },
                {
                    "reponse_time_8": 3.9145429134368896
                },
                {
                    "chargement_model": 39.79385781288147
                },
                {
                    "reponse_time_9": 3.7057714462280273
                },
                {
                    "avg_reponse_time": 4.023995108074612
                },
                {
                    "total_time": 39.79391527175903
                }
            ],
            "score": 9,
            "result": "[{'mot_cle': ['CNN LLMs', 'Convolutional Neural Networks Language Models', 'LLM Architecture Alternatives', 'Hybrid LLM Architectures', 'Limitations of CNNs in LLMs'], 'categories': 'Artificial Intelligence'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'transformer vs CNN language models', 'hybrid architectures LLMs'], 'categories': 'AI Research'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'alternative architectures LLMs', 'LLM design choices', 'transformer network architecture'], 'categories': 'Artificial Intelligence'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'alternative architectures LLMs', 'hybrid models LLMs CNN', 'transformer architecture advantages'], 'categories': 'AI Research'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'transformer networks vs CNNs', 'hybrid architectures LLMs'], 'categories': 'AI Research'}, {'mot_cle': ['CNN limitations LLMs', 'Convolutional Neural Networks vs. LLMs', 'LLM architecture alternatives', 'Transformer networks advantages', 'Hybrid architectures LLMs CNN'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['CNN LLMs', 'Convolutional Neural Networks Language Models', 'LLM architecture alternatives', 'Why not CNN for LLMs', 'Limitations of CNN in LLMs'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architectures alternatives', 'transformer network advantages', 'hybrid CNN-Transformer models'], 'categories': 'AI Research'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'hybrid architectures LLMs CNN', 'transformer networks vs CNN'], 'categories': 'AI Research'}]"
        },
        {
            "model_name": "gemma-3-12b-it",
            "temperature": 0.7,
            "max_tokens": 4096,
            "attempts": 10,
            "time": [
                {
                    "chargement_model": 3.850515604019165
                },
                {
                    "reponse_time_0": 3.850515127182007
                },
                {
                    "chargement_model": 7.786626815795898
                },
                {
                    "reponse_time_1": 3.936061382293701
                },
                {
                    "chargement_model": 11.143741369247437
                },
                {
                    "chargement_model": 15.209341049194336
                },
                {
                    "chargement_model": 18.846089839935303
                },
                {
                    "chargement_model": 23.474012851715088
                },
                {
                    "reponse_time_5": 4.627868175506592
                },
                {
                    "chargement_model": 27.893250465393066
                },
                {
                    "reponse_time_6": 4.419196128845215
                },
                {
                    "chargement_model": 32.305365562438965
                },
                {
                    "reponse_time_7": 4.412046909332275
                },
                {
                    "chargement_model": 37.01218914985657
                },
                {
                    "reponse_time_8": 4.706765651702881
                },
                {
                    "chargement_model": 40.37133169174194
                },
                {
                    "avg_reponse_time": 4.325408895810445
                },
                {
                    "total_time": 40.371381521224976
                }
            ],
            "score": 6,
            "result": "[{'mot_cle': ['CNN LLMs', 'convolutional neural networks language models', 'LLM architecture limitations', 'alternative LLM designs', 'hybrid neural network models'], 'categories': 'AI Research'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'hybrid architectures LLMs CNN', 'transformer network advantages'], 'categories': 'AI Research'}, {'mot_cle': ['CNN for LLMs', 'Convolutional Neural Networks Language Models', 'LLM Architecture Alternatives', 'Why not CNN in LLMs', 'Limitations of CNNs in Large Language Models'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['CNN for LLMs', 'Convolutional Neural Networks Language Models', 'LLM architecture limitations', 'Alternatives to Transformers in LLMs', 'Hybrid CNN-Transformer models'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations language models', 'LLM architectures alternatives', 'hybrid CNN-Transformer models', 'why not CNN for LLMs'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['CNN for LLMs', 'Convolutional Neural Networks Language Models', 'Limitations of CNNs in LLMs', 'Alternatives to CNNs in LLMs', 'Transformer Architecture vs CNNs'], 'categories': 'Artificial Intelligence, Natural Language Processing'}]"
        },
        {
            "model_name": "gemma-3-12b-it",
            "temperature": 0.7,
            "max_tokens": 4096,
            "attempts": 10,
            "time": [
                {
                    "chargement_model": 3.8455851078033447
                },
                {
                    "chargement_model": 7.968020915985107
                },
                {
                    "reponse_time_1": 4.122391223907471
                },
                {
                    "chargement_model": 11.601735353469849
                },
                {
                    "reponse_time_2": 3.633690595626831
                },
                {
                    "chargement_model": 15.232299566268921
                },
                {
                    "chargement_model": 19.0083429813385
                },
                {
                    "reponse_time_4": 3.775986433029175
                },
                {
                    "chargement_model": 23.37508988380432
                },
                {
                    "reponse_time_5": 4.366702079772949
                },
                {
                    "chargement_model": 27.52297019958496
                },
                {
                    "chargement_model": 31.9596745967865
                },
                {
                    "reponse_time_7": 4.436656951904297
                },
                {
                    "chargement_model": 36.16936707496643
                },
                {
                    "reponse_time_8": 4.209661483764648
                },
                {
                    "chargement_model": 40.02596974372864
                },
                {
                    "reponse_time_9": 3.856576919555664
                },
                {
                    "avg_reponse_time": 4.057380812508719
                },
                {
                    "total_time": 40.02600908279419
                }
            ],
            "score": 7,
            "result": "[{'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'hybrid CNN-Transformer models', 'computational cost CNN LLMs'], 'categories': 'Artificial Intelligence'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'transformer network advantages', 'hybrid CNN-Transformer models'], 'categories': 'AI Research'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architectures alternatives', 'transformer vs CNN language models', 'hybrid neural network models'], 'categories': 'AI Research'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations language models', 'LLM architecture alternatives', 'hybrid architectures LLMs CNN', 'transformer network advantages LLMs'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['CNN LLMs', 'Convolutional Neural Networks Language Models', 'LLM Architecture Alternatives', 'Hybrid LLM Architectures', 'Limitations of CNNs in LLMs'], 'categories': 'Artificial Intelligence, Natural Language Processing'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'alternative architectures LLMs', 'LLM design considerations', 'transformer architecture vs CNN'], 'categories': 'Artificial Intelligence/Natural Language Processing'}, {'mot_cle': ['convolutional neural networks LLMs', 'CNN limitations LLMs', 'LLM architecture alternatives', 'transformer network advantages', 'hybrid architectures LLMs'], 'categories': 'AI Research'}]"
        },
        {
            "model_name": "gemma-3-12b-it",
            "temperature": 0.7,
            "max_tokens": 4096,
            "attempts": 10,
            "time": [
                {
                    "chargement_model": 4.418409585952759
                },
                {
                    "reponse_time_0": 4.418409109115601
                },
                {
                    "chargement_model": 8.839131832122803
                },
                {
                    "reponse_time_1": 4.420694351196289
                },
                {
                    "chargement_model": 12.472618103027344
                },
                {
                    "chargement_model": 16.401145935058594
                },
                {
                    "reponse_time_3": 3.9284937381744385
                },
                {
                    "chargement_model": 20.599753856658936
                },
                {
                    "reponse_time_4": 4.198567867279053
                },
                {
                    "chargement_model": 24.80635952949524
                },
                {
                    "chargement_model": 28.581992626190186
                },
                {
                    "chargement_model": 32.72793173789978
                },
                {
                    "reponse_time_7": 4.145872354507446
                },
                {
                    "chargement_model": 37.157078981399536
                },
                {
                    "reponse_time_8": 4.429107189178467
                },
                {
                    "chargement_model": 40.656670808792114
                },
                {
                    "avg_reponse_time": 4.256857434908549
                },
                {
                    "total_time": 40.6567223072052
                }
            ],
            "score": 6,
            "result": "[{'mot_cle': ['CNNforLLMs', 'ConvolutionalNeuralNetworksLanguageModels', 'LLMarchitecturelimitations', 'AlternativestoTransformersinLLMs', 'Hybridneuralnetworkmodelslanguage'], 'categories': 'ArtificialIntelligence,NaturalLanguageProcessing'}, {'mot_cle': ['CNNforLLMs', 'ConvolutionalNeuralNetworksLanguageModels', 'LimitationsofCNNsinLLMs', 'LLMArchitectureAlternatives', 'WhynotCNNsinLargeLanguageModels'], 'categories': 'AIResearch'}, {'mot_cle': ['CNNLLMs', 'ConvolutionalNeuralNetworksLanguageModels', 'LLMarchitecturelimitations', 'Hybridneuralnetworkmodels', 'AlternativearchitecturesforLLMs'], 'categories': 'ArtificialIntelligence'}, {'mot_cle': ['CNNforLLMs', 'ConvolutionalNeuralNetworksLanguageModels', 'LimitationsCNNinLLMs', 'AlternativestoCNNinLLMs', 'TransformervsCNNLLMs'], 'categories': 'AIResearch'}, {'mot_cle': ['CNNforLLMs', 'ConvolutionalNeuralNetworksvs.LLMs', 'LimitationsofCNNinLargeLanguageModels', 'AlternativearchitecturestoTransformers', 'Hybridneuralnetworkmodels'], 'categories': 'AIResearch'}, {'mot_cle': ['CNNLLMs', 'ConvolutionalNeuralNetworksLanguageModels', 'LLMarchitecturescomparison', 'WhynotCNNforLLMs', 'LimitationsofCNNinLLMs'], 'categories': 'ArtificialIntelligence,NaturalLanguageProcessing'}]"
        },
        {
            "model_name": "gemma-3-12b-it",
            "temperature": 0.7,
            "max_tokens": 4096,
            "attempts": 10,
            "time": [
                {
                    "chargement_model": 4.3342976570129395
                },
                {
                    "reponse_time_0": 4.334296941757202
                },
                {
                    "chargement_model": 8.247980117797852
                },
                {
                    "reponse_time_1": 3.9136550426483154
                },
                {
                    "chargement_model": 12.29635214805603
                },
                {
                    "reponse_time_2": 4.0483293533325195
                },
                {
                    "chargement_model": 16.219852209091187
                },
                {
                    "reponse_time_3": 3.923466444015503
                },
                {
                    "chargement_model": 20.130253076553345
                },
                {
                    "reponse_time_4": 3.9103596210479736
                },
                {
                    "chargement_model": 24.19557476043701
                },
                {
                    "reponse_time_5": 4.065286874771118
                },
                {
                    "chargement_model": 27.969488859176636
                },
                {
                    "reponse_time_6": 3.7738516330718994
                },
                {
                    "chargement_model": 31.61319923400879
                },
                {
                    "chargement_model": 35.953097343444824
                },
                {
                    "reponse_time_8": 4.339845657348633
                },
                {
                    "chargement_model": 40.02698588371277
                },
                {
                    "reponse_time_9": 4.073846817016602
                },
                {
                    "avg_reponse_time": 4.04254870944553
                },
                {
                    "total_time": 40.027016401290894
                }
            ],
            "score": 9,
            "result": "[{'mot_cle': ['CNNLLMs', 'ConvolutionalNeuralNetworksLanguageModels', 'LLMArchitectureAlternatives', 'HybridLLMArchitectures', 'LimitationsofCNNsinLLMs'], 'categories': 'ArtificialIntelligence,NaturalLanguageProcessing'}, {'mot_cle': ['convolutionalneuralnetworksLLMs', 'CNNlimitationsLLMs', 'LLMarchitecturesalternatives', 'transformernetworksvsCNNs', 'hybridneuralnetworkmodels'], 'categories': 'AIResearch'}, {'mot_cle': ['convolutionalneuralnetworksLLMs', 'CNNlimitationsLLMs', 'LLMarchitecturesalternatives', 'hybridCNN-Transformermodels', 'visuallanguagemodelsvsLLMs'], 'categories': 'AIResearch'}, {'mot_cle': ['convolutionalneuralnetworksLLMs', 'CNNlimitationsLLMs', 'LLMarchitecturealternatives', 'transformervsCNNlanguagemodels', 'hybridarchitecturesLLMs'], 'categories': 'AIResearch'}, {'mot_cle': ['CNNforLLMs', 'ConvolutionalNeuralNetworksLanguageModels', 'LLMarchitecturelimitations', 'AlternativearchitecturestoTransformers', 'HybridCNN-Transformermodels'], 'categories': 'AIResearch'}, {'mot_cle': ['CNNforLLMs', 'ConvolutionalNeuralNetworksLanguageModels', 'LLMarchitecturelimitations', 'AlternativestoTransformersinLLMs', 'HybridCNN-Transformermodels'], 'categories': 'AIResearch'}, {'mot_cle': ['convolutionalneuralnetworksLLMs', 'CNNlimitationsLLMs', 'alternativearchitecturesLLMs', 'LLMdesignchoices', 'transformernetworkadvantages'], 'categories': 'AIResearch'}, {'mot_cle': ['convolutionalneuralnetworksLLMs', 'CNNlimitationsLLMs', 'LLMarchitecturesalternatives', 'hybridCNN-Transformermodels', 'whynotCNNforlanguagemodeling'], 'categories': 'ArtificialIntelligence,NaturalLanguageProcessing'}, {'mot_cle': ['convolutionalneuralnetworksLLMs', 'CNNlimitationsLLMs', 'LLMarchitecturesalternatives', 'hybridCNN-Transformermodels', 'whynotCNNforlanguagemodels'], 'categories': 'AIResearch'}]"
        }
    ]
}