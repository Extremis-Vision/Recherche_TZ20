from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from typing import List, Optional
from dotenv import load_dotenv
import os
import lmstudio as lms

def response_with_context(prompt: str, context: str, model_name: str = "ministral-8b-instruct-2410", language : str = "French" ) -> Optional[str]:
    """
    Génère une réponse structurée à partir d'un prompt utilisateur et d'un contexte, en utilisant un output parser.
    """

    system_prompt = f"""
            You must generate a response using only the context provided below, and you must cite the sources of any information you use.
            Your response must be in the same language as the user's input here {language}.
            Citation format:
            At the end of each paragraph that uses information from a source, add the following citation format: (source_name)[link]

            Example:
            (datascientest.com)[https://datascientest.com/transformer-models-tout-savoir]

            Instructions:
            - Use only the provided context to answer the user's question.
            - Do not use any external information or sources.
            - For every factual statement or paragraph that uses information from the context, cite the relevant source in the specified format.
            - Write your response in the same language as the user's input here {language}.
            - Do not invent or hallucinate sources.
            - Only respond to the user query no more no less, but always source what you say in the user query language.
            - If your not capable to respond correctly just says what you lack of.

            Context to use:
            {context}
            """

    

    try:
        model = lms.llm(model_name)
        chat = lms.Chat(system_prompt)
        chat.add_user_message(prompt)

        prediction_stream = model.respond_stream(chat)

        print("Bot :", end=" ", flush=True)
        for fragment in prediction_stream:
            print(fragment.content, end="", flush=True)
        print()
        while True:
            user_input = input("Vous (laisser vide pour quitter) : ")
            if not user_input:
                break
            chat.add_user_message(user_input)

            # Lancement du streaming de la réponse
            prediction_stream = model.respond_stream(chat)

            print("Bot :", end=" ", flush=True)
            for fragment in prediction_stream:
                print(fragment.content, end="", flush=True)
            print()



    except Exception as e:
        print(f"Erreur lors de la génération de la réponse : {e}")
        return None
    

response_with_context("Quels est la météo à Seppois", "La météo à Seppois est assez gris une température de 13C° avec un peu ded pluie. LA météo à rio est ensoleilé ")